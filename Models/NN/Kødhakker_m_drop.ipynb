{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kødhakker_m_drop.ipynb","version":"0.3.2","provenance":[{"file_id":"1Kw22ps_PNAhnhh9q6tiIHDOSCf6rYAT-","timestamp":1559591098890}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"rZ6swUWr5O4Z","colab_type":"code","colab":{}},"source":["!pip install tensorflow==2.0.0-alpha0\n","!pip install tensorflow-gpu==2.0.0-alpha0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NBOl7dY95Ki9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c7f3b788-1db5-4cd2-cacc-7326eaaa0041","executionInfo":{"status":"ok","timestamp":1559591626165,"user_tz":-120,"elapsed":2919,"user":{"displayName":"Sebastian Pedersen","photoUrl":"","userId":"07014137639142725140"}}},"source":["import tensorflow as tf\n","print(\"TensorFlow version:\", tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow version: 2.0.0-alpha0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BOYqj0FunP1T","colab_type":"code","outputId":"2f420681-a82d-498d-b223-988d1835d85c","executionInfo":{"status":"ok","timestamp":1559591675586,"user_tz":-120,"elapsed":47063,"user":{"displayName":"Sebastian Pedersen","photoUrl":"","userId":"07014137639142725140"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","\n","import os\n","import sys\n","\n","for root, dirs, files in os.walk('/gdrive', topdown=False):\n","  for name in dirs:\n","      if name.lower() == \"lsda\":  # Assumes folder named 'LSDA' to exsist, and contains the repo and data\n","        sys.path.append(os.path.join(root, name))\n","\n","from General.Paths import Data_Path"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bHaPDOjMnIAY","colab_type":"code","colab":{}},"source":["from _0_DataCreation.Read_Data import load_dataframe\n","from General.Paths import Gitlab_Path\n","from Scoring.scoring_func import f1_scores_plot\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cW-H5Tk2nXYs","colab_type":"code","outputId":"44ff09b5-3adc-4253-a03d-9a639ed4aa63","executionInfo":{"status":"ok","timestamp":1559591825168,"user_tz":-120,"elapsed":24541,"user":{"displayName":"Sebastian Pedersen","photoUrl":"","userId":"07014137639142725140"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["### Import and reshape data ----------------------------------------------------------------------\n","fold1_df = load_dataframe(filename = 'fold1_NA_features.dat')\n","fold2_df = load_dataframe(filename = 'fold2_NA_features.dat')\n","fold3_df = load_dataframe(filename = 'fold3_NA_features.dat')\n","\n","del fold1_df['id']\n","del fold2_df['id']\n","del fold3_df['id']\n","\n","dw_cols = [x for x in fold1_df.columns if x[-2:] == 'dw' and x[:3] == 'pca']\n","fold1_df[dw_cols] = np.log(np.array(fold1_df[dw_cols]))\n","fold2_df[dw_cols] = np.log(np.array(fold2_df[dw_cols]))\n","fold3_df[dw_cols] = np.log(np.array(fold3_df[dw_cols]))\n","fold1_df = fold1_df.replace([-np.inf],0)\n","fold2_df = fold2_df.replace([-np.inf],0)\n","fold3_df = fold3_df.replace([-np.inf],0)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Size: 66.77Mb\n","Size: 80.44Mb\n","Size: 23.49Mb\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rUhE9QUOnbfj","colab_type":"code","colab":{}},"source":["### Create model ----------------------------------------------------------------------\n","input_length = len(fold1_df.columns) -1 #not label\n","\n","inputs = tf.keras.Input(shape = (input_length,))\n","\n","L1 = tf.keras.layers.Dense(128, activation = 'sigmoid', kernel_initializer=tf.initializers.VarianceScaling(scale=0.01**2))(inputs)\n","L12 = tf.keras.layers.Dropout(0.5)(L1)\n","\n","L2 = tf.keras.layers.Dense(128,activation = 'sigmoid', kernel_initializer=tf.initializers.VarianceScaling(scale=0.01**2))(L12)\n","L22 = tf.keras.layers.Dropout(0.5)(L2)\n","\n","L3 = tf.keras.layers.Dense(64,activation = 'sigmoid', kernel_initializer=tf.initializers.VarianceScaling(scale=0.01**2))(L22)\n","L32 = tf.keras.layers.Dropout(0.4)(L3)\n","\n","\n","conc_layer = tf.keras.layers.concatenate([inputs,L3])\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.initializers.VarianceScaling(scale=0.01**2))(conc_layer)\n","nn_model = tf.keras.Model(inputs=inputs, outputs=output)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGy7kEeAngCz","colab_type":"code","colab":{}},"source":["# Define optimization algorithm\n","ada = tf.optimizers.Adam(lr=0.00015, decay=0.00003) #Sikkert fint\n","\n","# Compile model (i.e., build compute graph)\n","nn_model.compile(optimizer=ada,\n","              loss='binary_crossentropy')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALHRxLU1nlxn","colab_type":"code","outputId":"1a11b213-60e4-4a94-f2d4-c95651d70fc7","executionInfo":{"status":"ok","timestamp":1559594609070,"user_tz":-120,"elapsed":592762,"user":{"displayName":"Sebastian Pedersen","photoUrl":"","userId":"07014137639142725140"}},"colab":{"base_uri":"https://localhost:8080/","height":3128}},"source":["training_sets = [['fold1','fold2'],['fold1','fold3'],['fold2','fold3']]\n","val_set = ['fold3','fold2','fold1']\n","\n","\n","# Training loop\n","x_train1 = pd.concat([fold1_df[[x for x in fold1_df.columns if x != 'label']],fold2_df[[x for x in fold2_df.columns if x != 'label']]])\n","x_train2 = pd.concat([fold1_df[[x for x in fold1_df.columns if x != 'label']],fold3_df[[x for x in fold3_df.columns if x != 'label']]])\n","x_train3 = pd.concat([fold2_df[[x for x in fold2_df.columns if x != 'label']],fold3_df[[x for x in fold3_df.columns if x != 'label']]])\n","\n","y_train1 = pd.concat([fold1_df['label'],fold2_df['label']])\n","y_train2 = pd.concat([fold1_df['label'],fold3_df['label']])\n","y_train3 = pd.concat([fold2_df['label'],fold3_df['label']])\n","\n","x_trains = [x_train1,x_train2,x_train3]\n","y_trains = [y_train1,y_train2,y_train3]\n","\n","x_val1 = fold3_df[[x for x in fold3_df.columns if x != 'label']]\n","x_val2 = fold2_df[[x for x in fold2_df.columns if x != 'label']]\n","x_val3 = fold1_df[[x for x in fold1_df.columns if x != 'label']]\n","\n","y_val1 = fold3_df['label']\n","y_val2 = fold2_df['label']\n","y_val3 = fold1_df['label']\n","\n","x_vals = [x_val1,x_val2,x_val3]\n","y_vals = [y_val1,y_val2,y_val3]\n","\n","for i in range(3):\n","  \n","  x_train = x_trains[i]\n","  y_train = y_trains[i]\n","  x_val = x_vals[i]\n","  y_val = y_vals[i]\n","  callbacks = [\n","    tf.keras.callbacks.ModelCheckpoint('./exciting_' + str(i) + '.hdf5', monitor='val_loss', save_best_only=True, verbose=1)\n","  ]\n","\n","  nn_model.fit(x_train, y_train, batch_size=32, epochs=15, \n","            validation_data=(x_val, y_val), validation_freq=1, \n","            #steps_per_epoch=x_train.shape[0],\n","            callbacks=callbacks)\n","\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Train on 169254 samples, validate on 27006 samples\n","Epoch 1/15\n","169216/169254 [============================>.] - ETA: 0s - loss: 0.2568\n","Epoch 00001: val_loss improved from inf to 0.24412, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 111us/sample - loss: 0.2568 - val_loss: 0.2441\n","Epoch 2/15\n","168896/169254 [============================>.] - ETA: 0s - loss: 0.2360\n","Epoch 00002: val_loss improved from 0.24412 to 0.24319, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 112us/sample - loss: 0.2360 - val_loss: 0.2432\n","Epoch 3/15\n","169184/169254 [============================>.] - ETA: 0s - loss: 0.2339\n","Epoch 00003: val_loss improved from 0.24319 to 0.24284, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 111us/sample - loss: 0.2339 - val_loss: 0.2428\n","Epoch 4/15\n","168864/169254 [============================>.] - ETA: 0s - loss: 0.2328\n","Epoch 00004: val_loss improved from 0.24284 to 0.24106, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 112us/sample - loss: 0.2327 - val_loss: 0.2411\n","Epoch 5/15\n","169248/169254 [============================>.] - ETA: 0s - loss: 0.2317\n","Epoch 00005: val_loss did not improve from 0.24106\n","169254/169254 [==============================] - 18s 109us/sample - loss: 0.2317 - val_loss: 0.2412\n","Epoch 6/15\n","169152/169254 [============================>.] - ETA: 0s - loss: 0.2310\n","Epoch 00006: val_loss improved from 0.24106 to 0.24097, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 111us/sample - loss: 0.2310 - val_loss: 0.2410\n","Epoch 7/15\n","168928/169254 [============================>.] - ETA: 0s - loss: 0.2305\n","Epoch 00007: val_loss improved from 0.24097 to 0.23968, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 112us/sample - loss: 0.2304 - val_loss: 0.2397\n","Epoch 8/15\n","168960/169254 [============================>.] - ETA: 0s - loss: 0.2298\n","Epoch 00008: val_loss improved from 0.23968 to 0.23921, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 112us/sample - loss: 0.2298 - val_loss: 0.2392\n","Epoch 9/15\n","168992/169254 [============================>.] - ETA: 0s - loss: 0.2295\n","Epoch 00009: val_loss improved from 0.23921 to 0.23911, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 20s 116us/sample - loss: 0.2295 - val_loss: 0.2391\n","Epoch 10/15\n","168928/169254 [============================>.] - ETA: 0s - loss: 0.2291\n","Epoch 00010: val_loss improved from 0.23911 to 0.23897, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 20s 120us/sample - loss: 0.2291 - val_loss: 0.2390\n","Epoch 11/15\n","168928/169254 [============================>.] - ETA: 0s - loss: 0.2289\n","Epoch 00011: val_loss improved from 0.23897 to 0.23860, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 20s 116us/sample - loss: 0.2289 - val_loss: 0.2386\n","Epoch 12/15\n","169248/169254 [============================>.] - ETA: 0s - loss: 0.2285\n","Epoch 00012: val_loss did not improve from 0.23860\n","169254/169254 [==============================] - 20s 116us/sample - loss: 0.2285 - val_loss: 0.2388\n","Epoch 13/15\n","169248/169254 [============================>.] - ETA: 0s - loss: 0.2285\n","Epoch 00013: val_loss improved from 0.23860 to 0.23823, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 115us/sample - loss: 0.2285 - val_loss: 0.2382\n","Epoch 14/15\n","168928/169254 [============================>.] - ETA: 0s - loss: 0.2282\n","Epoch 00014: val_loss did not improve from 0.23823\n","169254/169254 [==============================] - 21s 125us/sample - loss: 0.2282 - val_loss: 0.2384\n","Epoch 15/15\n","169184/169254 [============================>.] - ETA: 0s - loss: 0.2280\n","Epoch 00015: val_loss improved from 0.23823 to 0.23794, saving model to ./exciting_0.hdf5\n","169254/169254 [==============================] - 19s 115us/sample - loss: 0.2280 - val_loss: 0.2379\n","Train on 103779 samples, validate on 92481 samples\n","Epoch 1/15\n","103648/103779 [============================>.] - ETA: 0s - loss: 0.2403\n","Epoch 00001: val_loss improved from inf to 0.21654, saving model to ./exciting_1.hdf5\n","103779/103779 [==============================] - 16s 152us/sample - loss: 0.2403 - val_loss: 0.2165\n","Epoch 2/15\n","103648/103779 [============================>.] - ETA: 0s - loss: 0.2396\n","Epoch 00002: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 144us/sample - loss: 0.2398 - val_loss: 0.2170\n","Epoch 3/15\n","103392/103779 [============================>.] - ETA: 0s - loss: 0.2397\n","Epoch 00003: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 143us/sample - loss: 0.2396 - val_loss: 0.2172\n","Epoch 4/15\n","103712/103779 [============================>.] - ETA: 0s - loss: 0.2390\n","Epoch 00004: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 142us/sample - loss: 0.2390 - val_loss: 0.2174\n","Epoch 5/15\n","103744/103779 [============================>.] - ETA: 0s - loss: 0.2391\n","Epoch 00005: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 145us/sample - loss: 0.2391 - val_loss: 0.2176\n","Epoch 6/15\n","103744/103779 [============================>.] - ETA: 0s - loss: 0.2387\n","Epoch 00006: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 16s 151us/sample - loss: 0.2387 - val_loss: 0.2174\n","Epoch 7/15\n","103776/103779 [============================>.] - ETA: 0s - loss: 0.2386\n","Epoch 00007: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 16s 152us/sample - loss: 0.2386 - val_loss: 0.2175\n","Epoch 8/15\n","103552/103779 [============================>.] - ETA: 0s - loss: 0.2381\n","Epoch 00008: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 16s 150us/sample - loss: 0.2381 - val_loss: 0.2176\n","Epoch 9/15\n","103616/103779 [============================>.] - ETA: 0s - loss: 0.2381\n","Epoch 00009: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 147us/sample - loss: 0.2382 - val_loss: 0.2178\n","Epoch 10/15\n","103584/103779 [============================>.] - ETA: 0s - loss: 0.2380\n","Epoch 00010: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 145us/sample - loss: 0.2380 - val_loss: 0.2177\n","Epoch 11/15\n","103776/103779 [============================>.] - ETA: 0s - loss: 0.2378\n","Epoch 00011: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 16s 151us/sample - loss: 0.2378 - val_loss: 0.2177\n","Epoch 12/15\n","103680/103779 [============================>.] - ETA: 0s - loss: 0.2377\n","Epoch 00012: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 148us/sample - loss: 0.2377 - val_loss: 0.2178\n","Epoch 13/15\n","103392/103779 [============================>.] - ETA: 0s - loss: 0.2378\n","Epoch 00013: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 147us/sample - loss: 0.2378 - val_loss: 0.2178\n","Epoch 14/15\n","103456/103779 [============================>.] - ETA: 0s - loss: 0.2378\n","Epoch 00014: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 145us/sample - loss: 0.2376 - val_loss: 0.2178\n","Epoch 15/15\n","103456/103779 [============================>.] - ETA: 0s - loss: 0.2373\n","Epoch 00015: val_loss did not improve from 0.21654\n","103779/103779 [==============================] - 15s 148us/sample - loss: 0.2373 - val_loss: 0.2179\n","Train on 119487 samples, validate on 76773 samples\n","Epoch 1/15\n","119008/119487 [============================>.] - ETA: 0s - loss: 0.2211\n","Epoch 00001: val_loss improved from inf to 0.23931, saving model to ./exciting_2.hdf5\n","119487/119487 [==============================] - 16s 132us/sample - loss: 0.2210 - val_loss: 0.2393\n","Epoch 2/15\n","119072/119487 [============================>.] - ETA: 0s - loss: 0.2201\n","Epoch 00002: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 132us/sample - loss: 0.2201 - val_loss: 0.2402\n","Epoch 3/15\n","119168/119487 [============================>.] - ETA: 0s - loss: 0.2196\n","Epoch 00003: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 17s 143us/sample - loss: 0.2197 - val_loss: 0.2407\n","Epoch 4/15\n","119328/119487 [============================>.] - ETA: 0s - loss: 0.2197\n","Epoch 00004: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 18s 148us/sample - loss: 0.2197 - val_loss: 0.2409\n","Epoch 5/15\n","119360/119487 [============================>.] - ETA: 0s - loss: 0.2197\n","Epoch 00005: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 138us/sample - loss: 0.2197 - val_loss: 0.2412\n","Epoch 6/15\n","119456/119487 [============================>.] - ETA: 0s - loss: 0.2191\n","Epoch 00006: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 138us/sample - loss: 0.2191 - val_loss: 0.2414\n","Epoch 7/15\n","119392/119487 [============================>.] - ETA: 0s - loss: 0.2191\n","Epoch 00007: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 136us/sample - loss: 0.2190 - val_loss: 0.2414\n","Epoch 8/15\n","119008/119487 [============================>.] - ETA: 0s - loss: 0.2192\n","Epoch 00008: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 135us/sample - loss: 0.2192 - val_loss: 0.2421\n","Epoch 9/15\n","119360/119487 [============================>.] - ETA: 0s - loss: 0.2187\n","Epoch 00009: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 137us/sample - loss: 0.2187 - val_loss: 0.2418\n","Epoch 10/15\n","119104/119487 [============================>.] - ETA: 0s - loss: 0.2189\n","Epoch 00010: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 18s 148us/sample - loss: 0.2188 - val_loss: 0.2420\n","Epoch 11/15\n","119424/119487 [============================>.] - ETA: 0s - loss: 0.2188\n","Epoch 00011: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 17s 144us/sample - loss: 0.2189 - val_loss: 0.2419\n","Epoch 12/15\n","119296/119487 [============================>.] - ETA: 0s - loss: 0.2185\n","Epoch 00012: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 136us/sample - loss: 0.2185 - val_loss: 0.2421\n","Epoch 13/15\n","119040/119487 [============================>.] - ETA: 0s - loss: 0.2185\n","Epoch 00013: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 134us/sample - loss: 0.2184 - val_loss: 0.2421\n","Epoch 14/15\n","119328/119487 [============================>.] - ETA: 0s - loss: 0.2185\n","Epoch 00014: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 133us/sample - loss: 0.2185 - val_loss: 0.2424\n","Epoch 15/15\n","119200/119487 [============================>.] - ETA: 0s - loss: 0.2185\n","Epoch 00015: val_loss did not improve from 0.23931\n","119487/119487 [==============================] - 16s 131us/sample - loss: 0.2183 - val_loss: 0.2424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YSBdj-29nrQC","colab_type":"code","outputId":"fb217ab3-e9e1-4620-8e79-7ffc5b0cc3e5","executionInfo":{"status":"ok","timestamp":1559594708183,"user_tz":-120,"elapsed":32141,"user":{"displayName":"Sebastian Pedersen","photoUrl":"","userId":"07014137639142725140"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["test_set_df = load_dataframe(filename = 'testSet_NA_features.dat')\n","\n","del test_set_df['id']\n","dw_cols = [x for x in test_set_df.columns if x[-2:] == 'dw' and x[:3] == 'pca']\n","test_set_df[dw_cols] = np.log(np.array(test_set_df[dw_cols]))\n","test_set_df = test_set_df.replace([-np.inf],0)\n","\n","predictions = []\n","\n","for i in range(3):\n","  print(i)\n","  #Get the best model\n","  best_model = nn_model\n","  best_model.load_weights('exciting_' + str(i) + '.hdf5')\n","\n","  preds = best_model.predict(test_set_df).flatten()\n","  predictions.append(preds)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Size: 149.59Mb\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n","  \"\"\"\n"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MMmdAko9oN8t","colab_type":"code","colab":{}},"source":["## Make the prediction ensemble on the testSet\n","all_preds = np.transpose(np.array(predictions))\n","my_y_preds = np.mean(all_preds,axis = 1)\n","\n","classifications = np.zeros(len(my_y_preds),dtype = int)\n","classifications[my_y_preds > 0.37] = 1\n","\n","my_df = pd.DataFrame({'Id':np.arange(1,len(classifications)+1),'ClassLabel':classifications})\n","my_df.to_csv(Gitlab_Path + '/Ranking/eat_snow3.csv',index = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMhwFasn0aNF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}